{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04w3OowXkaQh"
      },
      "source": [
        "# Build Your Neural Network In Cairo 1.0 with Orion\n",
        "\n",
        "Orion is a dedicated Cairo-based library designed specifically to build machine learning models for ValidityML. Its purpose is to facilitate verifiable inference. Orion exclusively operates with 8-bit quantized models, an approach intended to optimize performance. In this tutorial, you will be guided on how to train your model using Quantized Aware Training using MNIST dataset, how to convert your pre-trained model to Cairo 1, and how to perform inference with Orion."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install Dependecies, Rust, Cairo and Scarb\n",
        "Let's start by installing all dependecies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Sfci39Llvii"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install numpy tensorflow tensorflow_model_optimization matplotlib scipy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwTsquVDEe4p"
      },
      "outputs": [],
      "source": [
        "!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBsZQCRBExEc",
        "outputId": "54cbbc65-854e-4afa-b809-d2fc838f6ecf"
      },
      "outputs": [],
      "source": [
        "# Create .cairo folder if it doesn't exist yet\n",
        "! mkdir $HOME/.cairo\n",
        "\n",
        "! source \"$HOME/.cargo/env\"\n",
        "\n",
        "# Clone the Cairo compiler in $CAIRO_ROOT (default root)\n",
        "! cd $HOME/.cairo && git clone https://github.com/starkware-libs/cairo.git .\n",
        "\n",
        "# OPTIONAL/RECOMMENDED: If you want to install a specific version of the compiler\n",
        "# Fetch all tags (versions)\n",
        "! git fetch --all --tags\n",
        "# View tags (you can also do this in the cairo compiler repository)\n",
        "! git describe --tags `git rev-list --tags`\n",
        "# Checkout the version you want\n",
        "! git checkout tags/v1.1.0\n",
        "\n",
        "# Generate release binaries\n",
        "! cd $HOME/.cairo && $HOME/.cargo/bin/cargo build --all --release"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Scarb\n",
        "! curl --proto '=https' --tlsv1.2 -sSf https://docs.swmansion.com/scarb/install.sh | sh"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train and Test your Neural Network with Tensorflow\n",
        "In this section we will use Tensorflow to train and test a feedforward neural network with MNIST dataset."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nKiq8oKxklon"
      },
      "source": [
        "### Dataset Preparation\n",
        "Import the required libraries and load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4po2PWTAkWOR"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras.datasets import mnist\n",
        "from scipy.ndimage import zoom\n",
        "import numpy as np\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXigQHM_k0ux"
      },
      "source": [
        "We have a total of 70,000 grayscale images, each with a dimension of 28 x 28 pixels. 60,000 images are for training and the remaining 10,000 are for testing. \n",
        "\n",
        "We now need to pre-process our data. For the purposes of this tutorial and performance, we'll resize the images to 14 x 14 pixels. You'll see later that the neural network's input layer supports a 1D tensor. We, therefore, need to flatten and normalize our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6AwxawnnkaA5"
      },
      "outputs": [],
      "source": [
        "from scipy.ndimage import zoom\n",
        "\n",
        "# Resizing function\n",
        "def resize_images(images):\n",
        "    return np.array([zoom(image, 0.5) for image in images])\n",
        "\n",
        "# Resize\n",
        "x_train = resize_images(x_train)\n",
        "x_test = resize_images(x_test)\n",
        "\n",
        "# Then reshape\n",
        "x_train = x_train.reshape(60000, 14*14)\n",
        "x_test = x_test.reshape(10000, 14*14)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# normalize to range [0, 1]\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ix3qnUgElDlB"
      },
      "source": [
        "### Model Definition and Training\n",
        "We will design a straightforward feedforward neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gilDCS6k9-u"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "num_classes = 10\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.InputLayer(input_shape=(14*14,)),\n",
        "    keras.layers.Dense(10, activation='relu'), \n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='sparse_categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFUZZOudlQmP"
      },
      "source": [
        "Now let's train this model on our training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjkH102GlOd2"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "epochs = 10\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=epochs,\n",
        "                    validation_split=0.2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNRdKGpflar4"
      },
      "source": [
        "At this point, we have trained a regular model.\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7Bu55FCqlbj4"
      },
      "source": [
        "### Making the Model Quantization Aware\n",
        "Now, let's transform our model into a quantization aware model. We use the TensorFlow Model Optimization Toolkit for this.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAZYo9vKlTHK"
      },
      "outputs": [],
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "# Apply quantization to the layers\n",
        "quantize_model = tfmot.quantization.keras.quantize_model\n",
        "\n",
        "# q_aware stands for 'quantization aware'\n",
        "q_aware_model = quantize_model(model)\n",
        "\n",
        "# 'quantize_model' requires a recompile\n",
        "q_aware_model.compile(optimizer='adam',\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "q_aware_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYPUWWwTl_np"
      },
      "source": [
        "We have now created a new model, q_aware_model, which is a quantization aware version of our original model. Now we can train this model exactly like our original model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-t5MPhGlqoI"
      },
      "outputs": [],
      "source": [
        "batch_size = 256\n",
        "epochs = 10\n",
        "history = q_aware_model.fit(x_train, y_train,\n",
        "                            epochs=epochs,\n",
        "                            validation_split=0.2)\n",
        "\n",
        "scores, acc = q_aware_model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', scores)\n",
        "print('Test accuracy:', acc)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XhbweTQEmWN-"
      },
      "source": [
        "### Converting to TFLite Format\n",
        "Now, we will convert our model to TFLite format, which is a format optimized for on-device machine learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOwZiCRWmHDT"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Create a converter\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
        "\n",
        "# Indicate that you want to perform default optimizations,\n",
        "# which include quantization\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Define a generator function that provides your test data's numpy arrays\n",
        "def representative_data_gen():\n",
        "  for i in range(500):\n",
        "    yield [x_test[i:i+1]]\n",
        "\n",
        "# Use the generator function to guide the quantization process\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "# Ensure that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "\n",
        "# Set the input and output tensors to int8\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "# Convert the model\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(\"q_aware_model.tflite\", \"wb\").write(tflite_model)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "zxGWitSs3MAH"
      },
      "source": [
        "### Testing the Quantized Model\n",
        "Now that we have trained a quantization-aware model and converted it to the TFLite format, we can now perform inference using the TensorFlow Lite interpreter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qbfjJFa3Zy7"
      },
      "source": [
        "We first load the TFLite model and allocate the required tensors. The Interpreter class provides methods for loading a model and running inferences.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXEM3WEv3SSt"
      },
      "outputs": [],
      "source": [
        "# Load the TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_path=\"q_aware_model.tflite\")\n",
        "interpreter.allocate_tensors()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mP6WRo1L3dZp"
      },
      "source": [
        "Next, we get the details of the input and output tensors. Each tensor in a TensorFlow Lite model has a name, index, shape, data type, and quantization parameters. These can be accessed via the input_details and output_details methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7r8JFTC3Xxe"
      },
      "outputs": [],
      "source": [
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofFMDXq13ix7"
      },
      "source": [
        "Before performing the inference, we need to normalize the input to match the data type of our model's input tensor, which in our case is int8. Then, we use the set_tensor method to provide the input data to the model. We perform the inference using the invoke method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1yOJjl83b9i"
      },
      "outputs": [],
      "source": [
        "# Normalize the input value to int8\n",
        "input_shape = input_details[0]['shape']\n",
        "input_data = np.array(x_test[0:1], dtype=np.int8)\n",
        "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
        "\n",
        "# Perform the inference\n",
        "interpreter.invoke()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ge-Y3K1J3oWs"
      },
      "source": [
        "After the inference, we get the output data from the model's output tensor.\n",
        "\n",
        "Now, we are going to run the inference for the entire test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtfTBPCl3iKm"
      },
      "outputs": [],
      "source": [
        "# Get the result\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(output_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO9fwtAF3sRP"
      },
      "source": [
        "We normalize the entire test set and initialize an array to store the predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5fDA1Vt3num"
      },
      "outputs": [],
      "source": [
        "(_, _), (x_test_image, y_test_label) = mnist.load_data()\n",
        "\n",
        "# Resize and Normalize x_test_image to int8\n",
        "x_test_image = resize_images(x_test_image)\n",
        "x_test_image_norm = (x_test_image / 255.0 * 255 - 128).astype(np.int8)\n",
        "\n",
        "# Initialize an array to store the predictions\n",
        "predictions = []\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1PzXSuj3uy9"
      },
      "source": [
        "We then iterate over the test set, making predictions for each image. For each image, we flatten the image, normalize it, and then expand its dimensions to match the shape of our model's input tensor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1Clh4Kt3fuF"
      },
      "outputs": [],
      "source": [
        "# Iterate over the test data and make predictions\n",
        "for i in range(len(x_test_image_norm)):\n",
        "    test_image = np.expand_dims(x_test_image_norm[i].flatten(), axis=0)\n",
        "    \n",
        "    # Set the value for the input tensor\n",
        "    interpreter.set_tensor(input_details[0]['index'], test_image)\n",
        "    \n",
        "    # Run the inference\n",
        "    interpreter.invoke()\n",
        "\n",
        "    output = interpreter.get_tensor(output_details[0]['index'])\n",
        "    predictions.append(output)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we use a function to plot the test images along with their predicted labels. This will give us a visual representation of how well our model is performing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wWyve_E3uL8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_images_labels_prediction(images,labels,idx,num=10):\n",
        "    fig=plt.gcf()\n",
        "    fig.set_size_inches(12, 14)\n",
        "    if num > 25: num=25\n",
        "    for i in range(0, num):\n",
        "        ax=plt.subplot(5, 5, i+1)\n",
        "        ax.imshow(images[idx], cmap='binary')\n",
        "        title=\"label=\" + str(labels[idx])\n",
        "        ax.set_title(title, fontsize=10)\n",
        "        ax.set_xticks([]);\n",
        "        ax.set_yticks([]);\n",
        "        idx += 1\n",
        "    plt.show()\n",
        "\n",
        "plot_images_labels_prediction(x_test_image, y_test_label, 0, 25)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VOHKHUMp30j8"
      },
      "source": [
        "That's it! We have successfully trained a quantization-aware model, converted it to the TFLite format, and performed inference using the TensorFlow Lite interpreter."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ywhXM1mA-a7F"
      },
      "source": [
        "## Convert your model to Orion's Cairo code\n",
        "In this section you will generate Cairo files for each bias and weight of the model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wP_kVSuEKA1U"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "\n",
        "# Load the TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_path=\"q_aware_model.tflite\")\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Create an object with all tensors (an input + all weights and biases)\n",
        "tensors = {\n",
        "    \"input\": x_test_image[0].flatten(),\n",
        "    \"fc1_weights\": interpreter.get_tensor(1), \n",
        "    \"fc1_bias\": interpreter.get_tensor(2), \n",
        "    \"fc2_weights\": interpreter.get_tensor(4), \n",
        "    \"fc2_bias\": interpreter.get_tensor(5)\n",
        "}\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs('src/generated', exist_ok=True)\n",
        "\n",
        "for tensor_name, tensor in tensors.items():\n",
        "    with open(os.path.join('src', 'generated', f\"{tensor_name}.cairo\"), \"w\") as f:\n",
        "        f.write(\n",
        "            \"use array::ArrayTrait;\\n\" +\n",
        "            \"use orion::operators::tensor::core::{TensorTrait, Tensor, ExtraParams};\\n\" +\n",
        "            \"use orion::operators::tensor::implementations::impl_tensor_i32::Tensor_i32;\\n\" +\n",
        "            \"use orion::numbers::fixed_point::core::FixedImpl;\\n\" +\n",
        "            \"use orion::numbers::signed_integer::i32::i32;\\n\\n\" +\n",
        "            \"fn {0}() -> Tensor<i32> \".format(tensor_name) + \"{\\n\" +\n",
        "            \"    let mut shape = ArrayTrait::<usize>::new();\\n\"\n",
        "        )\n",
        "        for dim in tensor.shape:\n",
        "            f.write(\"    shape.append({0});\\n\".format(dim))\n",
        "        f.write(\n",
        "            \"    let mut data = ArrayTrait::<i32>::new();\\n\"\n",
        "        )\n",
        "        for val in np.nditer(tensor.flatten()):\n",
        "            f.write(\"    data.append(i32 {{ mag: {0}, sign: {1} }});\\n\".format(abs(int(val)), str(val < 0).lower()))\n",
        "        f.write(\n",
        "            \"let extra = ExtraParams { fixed_point: Option::Some(FixedImpl::FP16x16(())) }; \\n\" +\n",
        "            \"    TensorTrait::new(shape.span(), data.span(), Option::Some(extra))\\n\" +\n",
        "            \"}\\n\"\n",
        "        )\n",
        "      \n",
        "with open(os.path.join('src', 'generated.cairo'), 'w') as f:\n",
        "    for param_name in tensors.keys():\n",
        "        f.write(f\"mod {param_name};\\n\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iobaWNzdW4Jq"
      },
      "source": [
        "## Build your NN with Cairo and Orion\n",
        "In this section you will perform inference with Cairo and Orion.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the `nn.cairo` file in which we'll build the neural network functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2slaqnnUWlB"
      },
      "outputs": [],
      "source": [
        "! touch src/nn.cairo"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's create the two dense layer functions of the neural network: `fc1` and `fc2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMF0u2gcUko9"
      },
      "outputs": [],
      "source": [
        "%%writefile src/nn.cairo\n",
        "use orion::operators::tensor::core::Tensor;\n",
        "use orion::numbers::signed_integer::{integer_trait::IntegerTrait, i32::i32};\n",
        "use orion::operators::nn::core::NNTrait;\n",
        "use orion::numbers::fixed_point::core::FixedType;\n",
        "use orion::operators::nn::implementations::impl_nn_i32::NN_i32;\n",
        "\n",
        "fn fc1(i: Tensor<i32>, w: Tensor<i32>, b: Tensor<i32>) -> Tensor<i32> {\n",
        "    let x = NNTrait::linear(i, w, b);\n",
        "    NNTrait::relu(@x, IntegerTrait::new(0, false))\n",
        "}\n",
        "\n",
        "fn fc2(i: Tensor<i32>, w: Tensor<i32>, b: Tensor<i32>) -> Tensor<FixedType> {\n",
        "    let x = NNTrait::linear(i, w, b);\n",
        "    NNTrait::softmax(@x, 0)\n",
        "}"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will make predictions in a test. First, create the testing file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OF8ANN0IU0BL"
      },
      "outputs": [],
      "source": [
        "! touch src/test.cairo"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's now define the input data and parameters generated earlier, and set the neural network.\n",
        "The input data represents the number 7. The probability at index 7 must therefore be close to 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsWjlfyeWJF8"
      },
      "outputs": [],
      "source": [
        "%%writefile src/test.cairo\n",
        "use core::array::SpanTrait;\n",
        "\n",
        "use mnist_nn::nn::fc1;\n",
        "use mnist_nn::nn::fc2;\n",
        "use mnist_nn::generated::input::input;\n",
        "use mnist_nn::generated::fc1_bias::fc1_bias;\n",
        "use mnist_nn::generated::fc1_weights::fc1_weights;\n",
        "use mnist_nn::generated::fc2_bias::fc2_bias;\n",
        "use mnist_nn::generated::fc2_weights::fc2_weights;\n",
        "\n",
        "use orion::operators::tensor::implementations::impl_tensor_fp::Tensor_fp;\n",
        "\n",
        "#[test]\n",
        "#[available_gas(99999999999999999)]\n",
        "fn mnist_nn_test() {\n",
        "    let input = input();\n",
        "    let fc1_bias = fc1_bias();\n",
        "    let fc1_weights = fc1_weights();\n",
        "    let fc2_bias = fc2_bias();\n",
        "    let fc2_weights = fc2_weights();\n",
        "\n",
        "    let x = fc1(input, fc1_weights, fc1_bias);\n",
        "    let x = fc2(x, fc2_weights, fc2_bias);\n",
        "\n",
        "    let x = *x.argmax(0, Option::None(()), Option::None(())).data.at(0);\n",
        "\n",
        "    assert(x == 7, 'should predict 7');\n",
        "}\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the following cell to test your file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lkep4mVkWMtS"
      },
      "outputs": [],
      "source": [
        "! scarb run test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
